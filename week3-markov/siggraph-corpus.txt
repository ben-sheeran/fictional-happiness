We enrich character animations with secondary soft-tissue Finite Element Method (FEM) dynamics computed under arbitrary rigged or skeletal motion. Our method optionally incorporates pose-space deformation (PSD). It runs at milliseconds per frame for complex characters, and fits directly into standard character animation pipelines. Our simulation method does not require any skin data capture; hence, it can be applied to humans, animals, and arbitrary (real-world or fictional) characters. In standard model reduction of three-dimensional nonlinear solid elastic models, one builds a reduced model around a single pose, typically the rest configuration. We demonstrate how to perform multi-model reduction of Finite Element Method (FEM) nonlinear elasticity, where separate reduced models are precomputed around a representative set of object poses, and then combined at runtime into a single fast dynamic system, using subspace interpolation. While time-varying reduction has been demonstrated before for offline applications, our method is fast and suitable for hard real-time applications in games and virtual reality. Our method supports self-contact, which we achieve by computing linear modes and derivatives under contact constraints. Dynamic skin deformation is vital for creating life-like characters, and its real-time computation is in great demand in interactive applications. We propose a practical method to synthesize plausible and dynamic skin deformation based on a helper bone rig. This method builds helper bone controllers for the deformations caused not only by skeleton poses but also secondary dynamics effects. We introduce a state-space model for a discrete time linear time-invariant system that efficiently maps the skeleton motion to the dynamic movement of the helper bones. Optimal transfer of non-linear, complicated deformations, including the effect of soft-tissue dynamics, is obtained by learning the training sequence consisting of skeleton motions and corresponding skin deformations. Our approximation method for a dynamics model is highly accurate and efficient owing to its low-rank property obtained by a sparsity-oriented nuclear norm optimization. The resulting linear model is simple enough to easily implement in the existing workflows and graphics pipelines. We demonstrate the superior performance of our method compared to conventional dynamic skinning in terms of computational efficiency including LOD controls, stability in interactive controls, and flexible expression in deformations.Skinning algorithms that work across a broad range of character designs and poses are crucial to creating compelling animations. Currently, linear blend skinning (LBS) and dual quaternion skinning (DQS) are the most widely used, especially for real-time applications. Both techniques are efficient to compute and are effective for many purposes. However, they also have many well-known artifacts, such as collapsing elbows, candy wrapper twists, and bulging around the joints. Due to the popularity of LBS and DQS, it would be of great benefit to reduce these artifacts without changing the animation pipeline or increasing the computational cost significantly. In this paper, we introduce a new direct skinning method that addresses this problem. Our key idea is to pre-compute the optimized center of rotation for each vertex from the rest pose and skinning weights. At runtime, these centers of rotation are used to interpolate the rigid transformation for each vertex. Compared to other direct skinning methods, our method significantly reduces the artifacts of LBS and DQS while maintaining real-time performance and backwards compatibility with the animation pipeline. We present the first method to efficiently and accurately predict antialiasing footprints to pre-filter color-, normal-, and displacement-mapped appearance in the context of multi-bounce global illumination. We derive Fourier spectra for radiance and importance functions that allow us to compute spatial-angular filtering footprints at path vertices, for both uni- and bi-directional path construction. We then use these footprints to antialias reflectance modulated by high-resolution color, normal, and displacement maps encountered along a path. In doing so, we also unify the traditional path-space formulation of light-transport with our frequency-space interpretation of global illumination pre-filtering. Our method is fully compatible with all existing single bounce pre-filtering appearance models, not restricted by path length, and easy to implement atop existing path-space renderers. We illustrate its effectiveness on several radiometrically complex scenarios where previous approaches either completely fail or require orders of magnitude more time to arrive at similarly high-quality results. Specular BRDF rendering traditionally approximates surface microstructure using a smooth normal distribution, but this ignores glinty effects, easily observable in the real world. While modeling the actual surface microstructure is possible, the resulting rendering problem is prohibitively expensive. Recently, Yan et al. [2014] and Jakob et al. [2014] made progress on this problem, but their approaches are still expensive and lack full generality in their material and illumination support. We introduce an efficient and general method that can be easily integrated in a standard rendering system. We treat a specular surface as a four-dimensional position-normal distribution, and fit this distribution using millions of 4D Gaussians, which we call elements. This leads to closed-form solutions to the required BRDF evaluation and sampling queries, enabling the first practical solution to rendering specular microstructure. Rendering photo-realistic animal fur is a long-standing problem in computer graphics. Considerable effort has been made on modeling the geometric complexity of fur, but the reflectance of fur fibers is not well understood. Fur has a distinct diffusive and saturated appearance, that is not captured by either the Marschner hair model or the Kajiya-Kay model. In this paper, we develop a physically-accurate reflectance model for fur fibers. Based on anatomical literature and measurements, we develop a double cylinder model for the reflectance of a single fur fiber, where an outer cylinder represents the biological observation of a cortex covered by multiple cuticle layers, and an inner cylinder represents the scattering interior structure known as the medulla. Our key contribution is to model medulla scattering accurately --- in contrast, for human hair, the medulla has minimal width and thus negligible contributions to the reflectance. Medulla scattering introduces additional reflection and transmission paths, as well as diffusive reflectance lobes. We validate our physical model with measurements on real fur fibers, and introduce the first database in computer graphics of reflectance profiles for nine fur samples. We show that our model achieves significantly better fits to the measured data than the Marschner hair reflectance model. For efficient rendering, we develop a method to precompute 2D medulla scattering profiles and analytically approximate our reflectance model with factored lobes. The accuracy of the approach is validated by comparing our rendering model to full 3D light transport simulations. Our model provides an enriched set of controls, where the parameters we fit can be directly used to render realistic fur, or serve as a starting point from which artists can manually tune parameters for desired appearances. Shading with area lights adds a great deal of realism to CG renders. However, it requires solving spherical equations that make it challenging for real-time rendering. In this project, we develop a new spherical distribution that allows us to shade physically based materials with polygonal lights in real-time. Our idea is to start from a simple clamped cosine distribution and apply a linear transformation to its direction vectors. This allows for controlling the properties of the shape of the distribution, such as roughness, anisotropy, and skewness. Thanks to the variety of spherical shapes they cover, Linearly Transformed Cosines can closely approximate physically based BRDFs. Below is an example of how a GGX BRDF (left) can be approximated with a LTC (right) for varying incident directions. While Russian roulette (RR) and splitting are considered fundamental importance sampling techniques in neutron transport simulations, they have so far received relatively little attention in light transport. In computer graphics, RR and splitting are most often based solely on local reflectance properties. However, this strategy can be far from optimal in common scenes with non-uniform light distribution as it does not accurately predict the actual path contribution. In our approach, like in neutron transport, we estimate the expected contribution of a path as the product of the path weight and a pre-computed estimate of the adjoint transport solution. We use this estimate to generate so-called weight window which keeps the path contribution roughly constant through RR and splitting. As a result, paths in unimportant regions tend to be terminated early while in the more important regions they are spawned by splitting. This results in substantial variance reduction in both path tracing and photon tracing-based simulations. Furthermore, unlike the standard computer graphics RR, our approach does not interfere with importance-driven sampling of scattering directions, which results in superior convergence when such a technique is combined with our approach. We provide a justification of this behavior by relating our approach to the zero-variance random walk theory. Industrial knitting machines can produce finely detailed, seamless, 3D surfaces quickly and without human intervention. However, the tools used to program them require detailed manipulation and understanding of low-level knitting operations. We present a compiler that can automatically turn assemblies of high-level shape primitives (tubes, sheets) into low-level machine instructions. These high-level shape primitives allow knit objects to be scheduled, scaled, and otherwise shaped in ways that require thousands of edits to low-level instructions. At the core of our compiler is a heuristic transfer planning algorithm for knit cycles, which we prove is both sound and complete. This algorithm enables the translation of high-level shaping and scheduling operations into needle-level operations. We show a wide range of examples produced with our compiler and demonstrate a basic visual design interface that uses our compiler as a backend. Abstract: Fabrics play a significant role in many applications in design, prototyping, and entertainment. Recent fiber-based models capture the rich visual appearance of fabrics, but are too onerous to design and edit. Yarn-based procedural models are powerful and convenient, but too regular and not realistic enough in appearance. In this paper, we introduce an automatic fitting approach to create high-quality procedural yarn models of fabrics with fiber-level details. We fit CT data to procedural models to automatically recover a full range of parameters, and augment the models with a measurement-based model of flyaway fibers. We validate our fabric models against CT measurements and photographs, and demonstrate the utility of this approach for fabric modeling and editing. Modeling multiple scattering in microfacet theory is considered an important open problem because a non-negligible portion of the energy leaving rough surfaces is due to paths that bounce multiple times. In this paper, we are interested in pushing the Smith microsurface model forward and derive its missing multiple-scattering component. We are interested specifically in the Smith model because, thanks to its simple assumptions and its accurate single-scattering predictions, it has received widespread industrial adoption and is considered the academic state of the art in computer graphics. But can it be extended to multiple scattering? If multiple scattering can be incorporated into the Smith model, what does it look like? Does it remain accurate compared to reference data or does it break down? And can it be practically incorporated into a classic BSDF plugin? We introduce a Spatially-Varying BRDF model tailored to the multi-scale rendering of scratched materials such as metals, plastics or finished woods. Our approach takes advantage of the regular structure of scratch distributions to achieve high performance without compromising visual quality. We provide users with controls over the profile, micro-BRDF, density and orientation of scratches, while updating our material model at interactive rates. The BRDF for a single scratch is simulated using an optimized 2D ray-tracer and compactly stored in a three-component 2D texture. In contrast to existing models, our approach takes into account all inter-reflections inside a scratch, including Fresnel effects. At render time, the SV-BRDF for the scratch distribution under a pixel or ray footprint is obtained by linear combination of individual scratch BRDFs. We show how to evaluate it using both importance and light sampling, in direct and global illumination settings. We propose a novel surface-only technique for simulating incompressible, inviscid and uniform-density liquids with surface tension in three dimensions. The liquid surface is captured by a triangle mesh on which a Lagrangian velocity field is stored. Because advection of the velocity field may violate the incompressibility condition, we devise an orthogonal projection technique to remove the divergence while requiring the evaluation of only two boundary integrals. The forces of surface tension, gravity, and solid contact are all treated by a boundary element solve, allowing us to perform detailed simulations of a wide range of liquid phenomena, including waterbells, droplet and jet collisions, fluid chains, and crown splashes. We explore the question of whether phase-based time-of-flight (TOF) range cameras can be used for looking around corners and through scattering diffusers. By connecting TOF measurements with theory from array signal processing, we conclude that performance depends on two primary factors: camera modulation frequency and the width of the specular lobe (“shininess”) of the wall. For purely Lambertian walls, commodity TOF sensors achieve resolution on the order of meters between targets. For seemingly diffuse walls, such as posterboard, the resolution is drastically reduced, to the order of 10cm. In particular, we find that the relationship between reflectance and resolution is nonlinear—a slight amount of shininess can lead to a dramatic improvement in resolution. Since many realistic scenes exhibit a slight amount of shininess, we believe that off-the-shelf TOF cameras can look around corners. Diffractive optical elements (DOEs) have recently drawn great attention in computational imaging because they can drastically reduce the size and weight of imaging devices compared to their refractive counterparts. However, the inherent strong dispersion is a tremendous obstacle that limits the use of DOEs in full spectrum imaging, causing unacceptable loss of color fidelity in the images. In particular, metamerism introduces a data dependency in the image blur, which has been neglected in computational imaging methods so far. We introduce both a diffractive achromat based on computational optimization, as well as a corresponding algorithm for correction of residual aberrations. Using this approach, we demonstrate high fidelity color diffractive-only imaging over the full visible spectrum. In the optical design, the height profile of a diffractive lens is optimized to balance the focusing contributions of different wavelengths for a specific focal length. The spectral point spread functions (PSFs) become nearly identical to each other, creating approximately spectrally invariant blur kernels. This property guarantees good color preservation in the captured image and facilitates the correction of residual aberrations in our fast two-step deconvolution without additional color priors. We demonstrate our design of diffractive achromat on a 0.5mm ultrathin substrate by photolithography techniques. Experimental results show that our achromatic diffractive lens produces high color fidelity and better image quality in the full visible spectrum. Rendering translucent materials using Monte Carlo ray tracing is computationally expensive due to a large number of subsurface scattering events. Faster approaches are based on analytical models derived from diffusion theory. While such analytical models are efficient, they miss out on some translucency effects in the rendered result. We present an improved analytical model for subsurface scattering which captures translucency effects that are present in the reference solutions but remain absent with existing models. The key difference is that our model is based on ray source diffusion, rather than point source diffusion. A ray source corresponds better to the light that refracts through the surface of a translucent material. Using this ray source, we are able to take the direction of the incident light ray and the direction toward the point of emergence into account. We use a dipole construction similar to that of the standard dipole model, but we now have positive and negative ray sources with a mirrored pair of directions. Our model is as computationally efficient as existing models while it includes single scattering without relying on a separate Monte Carlo simulation, and the rendered images are significantly closer to the references. Unlike some previous work, our model is fully analytic and requires no precomputation. We address the problem of modeling and rendering granular materials—such as large structures made of sand, snow, or sugar—where an aggregate object is composed of many randomly oriented, but discernible grains. These materials pose a particular challenge as the complex scattering properties of individual grains, and their packing arrangement, can have a dramatic effect on the large-scale appearance of the aggregate object. We propose a multi-scale modeling and rendering framework that adapts to the structure of scattered light at different scales. We rely on path tracing the individual grains only at the finest scale, and—by decoupling individual grains from their arrangement—we develop a modular approach for simulating longer-scale light transport. We model light interactions within and across grains as separate processes and leverage this decomposition to derive parameters for classical radiative transport, including standard volumetric path tracing and a diffusion method that can quickly summarize the large scale transport due to many grain interactions. We require only a one-time precomputation per exemplar grain, which we can then reuse for arbitrary aggregate shapes and a continuum of different packing rates and scales of grains. We demonstrate our method on scenes containing mixtures of tens of millions of individual, complex, specular grains that would be otherwise infeasible to render with standard techniques. We introduce in this paper an algorithm that generates from an input tolerance volume a surface triangle mesh guaranteed to be within the tolerance, intersection free and topologically correct. A pliant meshing algorithm is used to capture the topology and discover the anisotropy in the input tolerance volume in order to generate a concise output. We first refine a 3D Delaunay triangulation over the tolerance volume while maintaining a piecewise-linear function on this triangulation, until an isosurface of this function matches the topology sought after. We then embed the isosurface into the 3D triangulation via mutual tessellation, and simplify it while preserving the topology. Our approach extends to surfaces with boundaries and to non-manifold surfaces. We demonstrate the versatility and efficacy of our approach on a variety of data sets and tolerance volumes. The Finite Element Method is widely used for solid deformable object simulation in film, computer games, virtual reality and medicine. Previous applications of nonlinear solid elasticity employed materials from a few standard families such as linear corotational, nonlinear St.Venant-Kirchhoff, Neo-Hookean, Ogden or Mooney-Rivlin materials. However, the spaces of all nonlinear isotropic and anisotropic materials are infinite-dimensional and much broader than these standard materials. In this paper, we demonstrate how to intuitively explore the space of isotropic and anisotropic nonlinear materials, for design of animations in computer graphics and related fields. In order to do so, we first formulate the internal elastic forces and tangent stiffness matrices in the space of the principal stretches of the material. We then demonstrate how to design new isotropic materials by editing a single stress-strain curve, using a spline interface. Similarly, anisotropic (orthotropic) materials can be designed by editing three curves, one for each material direction. We demonstrate that modifying these curves using our proposed interface has an intuitive, visual, effect on the simulation. Our materials accelerate simulation design and enable visual effects that are difficult or impossible to achieve with standard nonlinear materials. We present a method for simulating brittle fracture under the assumptions of quasi-static linear elastic fracture mechanics (LEFM). Using the boundary element method (BEM) and Lagrangian crack-fronts, we produce highly detailed fracture surfaces. The computational cost of the BEM is alleviated by using a low-resolution mesh and interpolating the resulting stress intensity factors when propagating the high-resolution crack-front. Our system produces physics-based fracture surfaces with high spatial and temporal resolution, taking spatial variation of material toughness and/or strength into account. It also allows for crack initiation to be handled separately from crack propagation, which is not only more reasonable from a physics perspective, but can also be used to control the simulation. Separating the resolution of the crack-front from the resolution of the computational mesh increases the efficiency and therefore the amount of visual detail on the resulting fracture surfaces. The BEM also allows us to re-use previously computed blocks of the system matrix. The challenge of shading food for Ratatouille was to achieve a stylized look for the world of Ratatouille, yet is still readable and recognizable as something appealing to eat. We, as humans, have a built-in sensory system to know what looks edible to our eyes and stomach. Finding that acceptable (and tasty) appearence was the main focus. To achieve this, we used subtle illumination techniques that became a general approach for a variety of objects. Here we will study a brief technical overview, followed by descriptions of different concepts, techniques and systems to achieve the look. Our food shading was based on skin and scatter computations, which have had internal improvements in speed and look since The Incredibles. The shaders were mainly a small variation on our in-house skin. We took full advantage of this technology base, since there is a breadth of research and precedents in the industry [Jensen, Marschner, Levoy, Hanrahan 2001] [Jenson, Buhler 2002].
Skin has always posed a challenge, since there is a familiarity to what the surface qualities should be. Food had similar problems when interpreting into cg, as well as sharing properties of translucency and a living organic feel (or post-life, in the case of food). For most shaders, we wanted to expediently feed the models into our pipeline quickly. We accomplished this by taking advantage of Slim, which offered a great level of flexibility. By creating networks that could input a variety of shading elements into our templates (procedural noises, textures, etc.), then using scatter on the results, we add an additional level of visual complexity to our otherwise “typical”, slim shaders.
For items which do not fit into this system easily (such as bread and wine), it became a “special case” shader that was refined on an individual development project basis. Once we found our general translucent look, the next step was to apply this to a variety of materials. We had objects to shade such as crusty bread, raw fish, cooked meats, fruits, vegetables, cheeses and sauces. There were several techniques and implementation details we discovered along the way.
Since there were often large ridges, dents and holes achieved through displacement, it was important to use the displaced position in our scatter grids, especially for the diffuse softening element. The result being we could push the intensity of displacements: furthermore, as they were captured inside the scatter grid, they were softened at render time. We also often used procedural and tiling textures over custom-painted textures per model. This allowed us to create interesting and complex patterns quickly, for reuse via slim. It became important to share these elements since we had many objects going from raw to cooked, or cooked to rotten. It would be prohibitively expensive to redo the work each time. The realization that the general shaders were the same across models (for example, with a grape and a tomato) and adapting a similiar shader for both, became part of our technique for dealing with the volume of models. Our cheese shaders were helped by strong scattering light, in addition to having the displacement component in the scatter grid to accentuate ridges and holes. An intense diffuse blur amount was used for cheeses and meats to increase the fleshy look. Pushing the diffuse blur, to the point where details were lost, was actually beneficial for a waxy cheese look.
Techniques, such as paint integrated with procedural shading and painted vertex varying data to control subsurface contribution, helped provide additional control in the shaders. Another key technique was using cellular textures, based on voronoi patterns to capture small details when added as a multiplier to scattering amount. This broke up the light transimission look, giving the effect of subcutaneous details, such as straition inside meat.
The different material properties in one model (like this steak dish below) required multiple scattering grids to avoid bleeding of irradiance across discreet food types. It was undesirable to have reddish steak as part of your light potato shader. The exception to this came when shading a rotten garbage pile, where this bleed became a positive side effect for blending the shaders, such as a fake bounce irradiance.
Another key to complexity was adding flecks of spices to our cooked dishes. Taking these flecks out of the diffuse blur was important, or else you would have blurry details rather than the desired parsley or pepper speck look. An oily or buttery coating for all cooked foods, was another level of detail. If the objects were cooked together, they would have a consistent coating. It was important to create that as a thin transparent layer on top of the shaders. After working on food for some time, we realized the method of starting with raw foods first was an easier approach to shading. It was more difficult to go from cooked to raw, or rotten to fresh. A lot of detail in the cooked versions were helped by modeling simulation softening and adding displacement. Often we reused the same shaders, with some small modifications; such as adding brushed details, cooking browning, or having our cooked colors move closer together toward a warm brownish hue. The bread was an example of a special case project that fell outside the general approach, requiring a new technique. It still used scattering to create a soft look, but took it one step further into visualizing this softness through the formation of air bubbles inside the density of the bread.
To create this airy effect we used a multi-step approach. The bread interior was a volumetric hypertexture (Perlin and Hoffert, 1989) defined using a procedural density function. The density function was made up of several layers of procedural voronoi noise. In addition, the important features -such as the position of the crust and position of the torn off face- were encoded in point clouds, which the shader used to modify the density function. The entire density function was also deformed using a texture map. The texture map was applied to the density field using a planar projection. Before evaluating the density function, the x and y positions of the current point were replaced by the value obtained from the texture map. This allowed us to compress air bubbles which were closer to the crust, then give a slight twist to the pattern of air bubbles, as often seen in real bread.